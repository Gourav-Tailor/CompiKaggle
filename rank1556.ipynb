{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# House Prices: Advanced Regression Techniques\nJoanna Broniarek\n\nBefore starting project I read some articles:\n1. https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n\n    + Select Data: Integrate data, de-normalize it into a dataset, collect it together.\n    + Preprocess Data: Format it, clean it, sample it so you can work with it.\n    + Transform Data: Feature Engineer happens here.\n    + Model Data: Create models, evaluate them and tune them.\n    + \n2. https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n\n2. 6 top factors  that influence a home’s value: https://www.inman.com/2017/08/07/6-factors-that-influence-a-homes-value/\n3. https://www.rochesterrealestateblog.com/what-factors-influence-the-sale-price-of-a-home/\n_______________\n\nI also took some inspiration and used some techniques of other people, but I did not hard-copied. ","metadata":{"id":"_kA8_JZronh_"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","metadata":{"id":"SA1KpvPToniC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import SimpleImputer, MissingIndicator\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder, Normalizer, StandardScaler, OneHotEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\nimport sklearn_pandas\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score,train_test_split\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.special import boxcox1p\nimport csv\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"W5nYYAP6oniF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport scipy\n\nprint('Environment specification:\\n')\nprint('python', '%s.%s.%s' % sys.version_info[:3])\n\nfor mod in np, scipy, sns, sklearn, pd:\n    print(mod.__name__, mod.__version__)","metadata":{"scrolled":true,"id":"YtsRRKAGoniH","outputId":"f3897833-a79b-40f4-b568-2ba7b4ce9d4f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading CSV file as Dataframe:","metadata":{"id":"taNOqRdJoniJ"}},{"cell_type":"code","source":"# kaggle train data\ndata_df = pd.read_csv(\"/content/train.csv\")\n# kaggle test data\ntest_df = pd.read_csv(\"/content/test.csv\")","metadata":{"id":"FMwYOr58oniK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = pd.concat((data_df.loc[:,:], test_df.loc[:, :]))\nall_data.head()","metadata":{"id":"2KTcGUlNoniM","outputId":"cdff6c6a-92ee-493c-c8de-b4a3f8267fd9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis and Data tidying","metadata":{"id":"xy9ED0ymoniO"}},{"cell_type":"markdown","source":"I noticed that the dataset consists of 38 numerical columns of 81 all. \nMorover, some columns contain missing data, i.e.\"LotFrontage\", \"MasVnrArea\".","metadata":{"id":"ob77vbyAoniQ"}},{"cell_type":"markdown","source":"**The prediction target is 'SalePrice'. Let's look closer to this column.**","metadata":{"id":"rW34x3WEoniU"}},{"cell_type":"code","source":"# Basic summary:\ndata_df['SalePrice'].describe()","metadata":{"id":"9tUhyjqmoniW","outputId":"cbb3ed59-613e-4105-cc62-e3cd7abbe2a7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data_df['SalePrice'])","metadata":{"id":"Zmlonk6HoniX","outputId":"0abfa841-5e90-4350-d14e-c7eca74a0f64"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two outliers with prices more than 700000.","metadata":{"id":"sKdvUP0FoniY"}},{"cell_type":"code","source":"# The Density Plot of SalePrice\nsns.distplot(data_df['SalePrice'])","metadata":{"id":"7kvmnMowoniZ","outputId":"4f89d8fd-d272-4ccd-e5d2-c307e48002ea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I noticed that it is right-skewed distribution with the pick around 160k and quite long tail with maximum about 800k.","metadata":{"id":"1jMJOxGMonia"}},{"cell_type":"code","source":"# Positive Skeweness:\ndata_df['SalePrice'].skew()","metadata":{"id":"UCjpUrx1onib","outputId":"aeb3098f-835e-40b5-f4fc-1bb598824beb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to be SalePrice more normal I do Logarithm transformation.","metadata":{"id":"aX8aChvuonib"}},{"cell_type":"code","source":"data_df[\"SalePrice\"] = np.log1p(data_df[\"SalePrice\"])","metadata":{"id":"vAxOx_w0onic"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SalePrice after Log-transformation\nsns.distplot(data_df[\"SalePrice\"])\nplt.title(\"Density plot of SalePrice after Log Transformation\")","metadata":{"id":"5tZDUh0xonic","outputId":"35e1432d-fcb6-42b0-b5bb-b9941a34ee71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = data_df[\"SalePrice\"]","metadata":{"id":"cM8-Obt2onid"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{"id":"VZXPHhzJonie"}},{"cell_type":"markdown","source":"#### Columns with Nan values\n\nAt first, I am checking the fraction of Nan values in each column. ","metadata":{"id":"93-7Fh9Bonif"}},{"cell_type":"code","source":"col_nan = data_df.isna().sum() / data_df.shape[0]","metadata":{"id":"2MIUD2rxonig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nsns.set(font_scale=1.2)\ncol_nan[col_nan > 0.01].plot(kind = \"barh\")\nplt.title(\"Features with the highest percentage of Nan values\")","metadata":{"id":"FGP8txidonih","outputId":"e54f0556-ff3c-447f-8fbe-79f1405ecd84"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" In some of my trials I removed columns with more than 90% od NaN values, \n    but finally I missed this step and left them for further analysis.\"\"\"\n\n# drop_out_columns = data_df.columns[[inx for inx, i in enumerate(col_nan >= 0.95) if i==True]]\n# data_df = data_df.drop(drop_out_columns, axis=1)\n# test_df = test_df.drop(drop_out_columns, axis=1)","metadata":{"id":"21RS29tvonii","outputId":"b4d4f33d-b4e2-4d4f-f1b9-5c5931f8072f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing Id column**","metadata":{"id":"_CNMJXxnonil"}},{"cell_type":"code","source":"# Dropping columns for both train and test dataset\ndata_df = data_df.drop(\"Id\", axis=1)\ntest_df = test_df.drop(\"Id\", axis=1)","metadata":{"id":"T_fO2KFYonil"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing columns that contain the same value in 100%**","metadata":{"id":"yN4AO8xAonim"}},{"cell_type":"markdown","source":"According to basic statistics provided on Kaggle competiton website, the columns Street and Utilities contain only one value \"Pave\" and \"AllPub\" respectively.","metadata":{"id":"6KaDhCXVonim"}},{"cell_type":"code","source":"data_df = data_df.drop([\"Street\", \"Utilities\"], axis=1)\ntest_df = test_df.drop([\"Street\", \"Utilities\"], axis=1)","metadata":{"id":"dYIs-S-bonin"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **Removing outliers**","metadata":{"id":"Qd3uHCRTonio"}},{"cell_type":"markdown","source":"Removing outliers is important step in data analysis. However, while removing outliers in ML we should be careful, because we do not know if there are not any outliers in test set.\n\nI used two techniques: more and less rigorous for this data.","metadata":{"id":"Wo-6p7dZonio"}},{"cell_type":"markdown","source":"The first one was **Z-score method**. Z-scores are expressed in terms of standard deviations from their means. As a result, these z-scores have a distribution with a mean of 0 and a standard deviation of 1.  \nI set **threshold = 3** to identify outliers.\n$$ z = \\frac{x - \\mu}{\\sigma} $$\n","metadata":{"id":"2kSzX9tqonip"}},{"cell_type":"code","source":"def remove_outliers(dataset, threshold, columns=None, removed = False):\n    \"\"\" \n    Z-score method.\n    Function returns a dataframe without rows labeled as 'outliers' according to the given threshold.  \n    ---------------\n    If columns = None, transform all numerical columns.\n    If removed = True, return also dataframe with removed rows.\n    \"\"\"\n    if columns==None:\n        numerics = ['int64','float64']\n        columns = dataset.select_dtypes(include=numerics).columns\n    \n    tmp = dataset.copy()\n    z = np.abs(stats.zscore(tmp[columns]))\n    outliers = [row.any() for row in (z > threshold)]  \n    outliers_idxs = tmp.index[outliers].tolist()\n    print(\"Number of removed rows = {}\".format(len(outliers_idxs)))\n    if removed: return dataset.drop(outliers_idxs), tmp.loc[outliers]\n    else: return dataset.drop(outliers_idxs)","metadata":{"id":"0YGDrYB5oniq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clear_data is a dataframe with train data after removing outliers\n\n# clear_data, removed_data = remove_outliers(data_df, threshold = 3, removed=True, \n#                              columns=['GrLivArea'])","metadata":{"id":"g96kf6THonir"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, after few trials I used less rigorous method. I just made a plot for SalePrice and GrLivArea and removed those which seems to be outliers.","metadata":{"id":"tQDNhHpyonir"}},{"cell_type":"markdown","source":"** Removing outliers according to GrLivArea**","metadata":{"id":"QEH4RiFHonis"}},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nsns.set(font_scale=1.2)\nsns.scatterplot(data_df[\"GrLivArea\"], data_df[\"SalePrice\"])\n# plt.vlines(4500, ymax=800000, ymin=0)\nplt.title(\"GrLivArea vs SalePrice\")","metadata":{"id":"YGy0UMPVonis","outputId":"79ae105c-a690-4550-c894-329db7e0e22d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I decided to remove those records where 'GrLivArea' is more than 4500. We can see on plot that they have a vey low price.\nclear_data = data_df.drop(data_df[(data_df['GrLivArea']>4500)].index)","metadata":{"id":"J6GA-DRBonit"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate all data together - both train and test\ntrain_ = clear_data.drop(['SalePrice'], axis=1)\nall_data = pd.concat([data_df, test_df]).reset_index(drop=True)","metadata":{"id":"mgzZeWqwoniu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GarageYrBlt feature**","metadata":{"id":"3KVZ1ix9oniv"}},{"cell_type":"markdown","source":"I checked if there are records that YearBuilt or GarageYrBlt have further year than 2017.","metadata":{"id":"j8Fe8CLfoniv"}},{"cell_type":"code","source":"print(\"Is there YearBuilt more than 2017 ? : \", all_data[all_data.YearBuilt > 2017].count()[0] != 0)\nprint(\"Is there GarageYrBlt more than 2017 ? : \", all_data[all_data.GarageYrBlt > 2017].count()[0] != 0)","metadata":{"id":"agoC5sp8oniw","outputId":"bca8bd86-1f53-48a1-ca93-a71ea4d7f39c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data[all_data.GarageYrBlt > 2017].GarageYrBlt #It seems like it is a typo","metadata":{"id":"5bTtMVlroniw","outputId":"d8816ec5-5373-4425-bc04-bb961974c0f0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.loc[2590, 'GarageYrBlt'] = 2007","metadata":{"id":"EkyH4oRyonix"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LotFrontage feature**","metadata":{"id":"Z6kKUzadonix"}},{"cell_type":"markdown","source":"LotFrontage is  a linear feet of street connected to property. I think it is a high probability that these values are similar to houses in the same Neighborhood. I check some statistics for them.","metadata":{"id":"YeCvpJwVonix"}},{"cell_type":"code","source":"neigh_lot_frontage = all_data.groupby('Neighborhood')['LotFrontage'].agg([\"mean\", \"median\"])\nneigh_lot_frontage['avg_mean_median'] = (neigh_lot_frontage['mean'] + neigh_lot_frontage['median'] )/ 2\nneigh_lot_frontage","metadata":{"id":"1TWI7fnqoniy","outputId":"d2514736-fbf0-40f0-d833-ab6c6ea43d46"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transformation into medians\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","metadata":{"id":"bhjs5tAvoniz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transformation of some numerical variables that are actually categorical.**","metadata":{"id":"K7vV3e6goniz"}},{"cell_type":"code","source":"def convert_to_string(df, columns):\n    df[columns] = df[columns].astype(str)\n    return df","metadata":{"id":"SCRML0wwoniz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_to_categ_features = ['MSSubClass', 'OverallCond']#, 'YrSold', 'MoSold']\n\nall_data = convert_to_string(all_data, columns = num_to_categ_features)","metadata":{"id":"Hg1cnemNoni0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Replacing missing values in the rest of numerical columns**\n\nFor the other numerical data I will also estimate them according to their statistics and for that I will use SimpleImputer object from sklearn library. For columns: BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, BsmtFullBath and BsmtHalfBath , MasVnrArea I will fill Nan values with constant = 0 and for the rest with median.","metadata":{"id":"Bu19-FgHoni0"}},{"cell_type":"code","source":"num_features = all_data.select_dtypes(include=['int64','float64']).columns\nnum_features_to_constant = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', \"MasVnrArea\"] \nnum_features_to_median = [feature for feature in num_features if feature not in num_features_to_constant + [\"SalePrice\"]]","metadata":{"id":"zgZPkDweoni0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating numerical features as input to DataFrameMapper.  \nnumeric_features_median = sklearn_pandas.gen_features(columns=[num_features_to_median], \n                                               classes=[{'class': SimpleImputer, \n                                                         'strategy': 'median', \n                                                         'missing_values' : np.nan}])\n\nnumeric_features_zero = sklearn_pandas.gen_features(columns=[num_features_to_constant], \n                                               classes=[{'class': SimpleImputer, \n                                                         'strategy': 'constant',\n                                                         'fill_value' : 0, \n                                                         'missing_values' : np.nan}])\n\nmissing_val_imputer = sklearn_pandas.DataFrameMapper(numeric_features_median + numeric_features_zero)\n\n# Fitting\nimputed_median = missing_val_imputer.fit(all_data)\n\n# Transformation\nimputed_features = imputed_median.transform(all_data)\n\n# Putting into dataframe\nimputed_df = pd.DataFrame(imputed_features, index=all_data.index, columns=num_features_to_median + num_features_to_constant)\n","metadata":{"id":"HIYe8t3Doni1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Categorical to numeral**","metadata":{"id":"yI-Jnq_soni1"}},{"cell_type":"markdown","source":"There is a lot of categorical features in data, so the next step is to transform  them into numerical values.","metadata":{"id":"t1m9s_i9oni1"}},{"cell_type":"code","source":"# Selecting category features\ncat_feats = all_data.select_dtypes(include=['object']).columns","metadata":{"id":"YZl6Md_woni1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to data description (possible values for each feature) on Kaggle, I created a list of conversion values, specific for each column.","metadata":{"id":"axu26hc3oni2"}},{"cell_type":"code","source":"none_conversion = [(\"MasVnrType\",\"None\"),\n                  (\"BsmtQual\",\"NA\"), \n                  (\"Electrical\", \"SBrkr\"),\n                  (\"BsmtCond\",\"TA\"),\n                  (\"BsmtExposure\",\"No\"),\n                  (\"BsmtFinType1\",\"No\"),\n                  (\"BsmtFinType2\",\"No\"),\n                  (\"CentralAir\",\"N\"),\n                  (\"Condition1\",\"Norm\"), \n                  (\"Condition2\",\"Norm\"),\n                  (\"ExterCond\",\"TA\"),\n                  (\"ExterQual\",\"TA\"), \n                  (\"FireplaceQu\",\"NA\"),\n                  (\"Functional\",\"Typ\"),\n                  (\"GarageType\",\"No\"), \n                  (\"GarageFinish\",\"No\"), \n                  (\"GarageQual\",\"NA\"), \n                  (\"GarageCond\",\"NA\"), \n                  (\"HeatingQC\",\"TA\"), \n                  (\"KitchenQual\",\"TA\"), \n                  (\"Functional\",\"Typ\"), \n                  (\"GarageType\",\"No\"), \n                  (\"GarageFinish\",\"No\"), \n                  (\"GarageQual\",\"No\"), \n                  (\"GarageCond\",\"No\"), \n                  (\"HeatingQC\",\"TA\"), \n                  (\"KitchenQual\",\"TA\"),\n                  (\"MSZoning\", \"None\"),\n                  (\"Exterior1st\", \"VinylSd\"), \n                  (\"Exterior2nd\", \"VinylSd\"), \n                  (\"SaleType\", \"WD\")]","metadata":{"id":"XvQ3r5veoni2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I implemented the none_transform function which converts missing categorical values into specific strings from the none_conversion dictionary.","metadata":{"id":"2HU2Pu5roni2"}},{"cell_type":"code","source":"def none_transform(df, conversion_list):\n    ''' Function that converts missing categorical values \n    into specific strings according to \"conversion_list\" \n    \n    Returns the dataframe after transformation.\n    '''\n    for col, new_str in conversion_list:\n        df.loc[:, col] = df.loc[:, col].fillna(new_str)\n    return df","metadata":{"id":"S-Apy-qXoni3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying the \"none_transform\" function \nall_data = none_transform(all_data, none_conversion)","metadata":{"id":"UXHzGrQKoni3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(all_data.columns)","metadata":{"id":"EjDuzbltoni4","outputId":"6cb73906-e246-4313-be4e-cf8f7cc97bf7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transformation of skewed features**","metadata":{"id":"6NCXBeW6oni4"}},{"cell_type":"markdown","source":"As for linear models preferable are normally distributed data, I am transforming the skewed features to make them more normally distributed.","metadata":{"id":"aAr1vItOoni4"}},{"cell_type":"code","source":"# collecting the numeric features without considering SalePrice\nnumeric_features = [feat for feat in num_features if feat not in ['SalePrice']] \n\n# selecting columns with skew more than 0.5\nskewed_features = all_data[num_features].apply(lambda x: x.dropna().skew())\nskewed_features = skewed_features[skewed_features > 0.5].index\nprint(\"\\nHighly skewed features: \\n\\n{}\".format(skewed_features.tolist()))","metadata":{"id":"fxYlauTjoni4","outputId":"810c6b8d-a5e9-4ede-eaf5-32ab701bd04c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I tried two method : Log-transformation and BoxCox transformation. As Box-Cox gave me little better results I decided to use it finally.","metadata":{"id":"-YUvrU8aoni5"}},{"cell_type":"code","source":"# Applying log-transformation \n# all_data[skewed_features] = np.log1p(all_data[skewed_features])# test_df[skewed_features] = np.log1p(test_df[skewed_features])","metadata":{"id":"w5L62PXWoni5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nA Box Cox transformation is a way to transform non-normal dependent variables into a normal shape.","metadata":{"id":"O-cvaRFGoni5"}},{"cell_type":"code","source":"#The “optimal lambda” is the one that results in the best approximation of a normal distribution curve. I selected lambda= 0.15.\n\nlambda_ = 0.15\nfor feature in skewed_features:\n    all_data[feature] = boxcox1p(all_data[feature], lambda_)","metadata":{"id":"wj0yQ2Z1oni6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Categorical into Numerical**\n","metadata":{"id":"0UlFLkM8oni6"}},{"cell_type":"markdown","source":"As some categorical features (i.e. **KitchenQual**, **GarageQual**) can be transformed into the numerical values with some order, I also implemented a new encoder for them. ","metadata":{"id":"oN81IM3Toni6"}},{"cell_type":"code","source":"class OrderedLabelTransformer(BaseEstimator, TransformerMixin):\n    orderDict = {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5}\n    \n    @staticmethod\n    def get_dict(X):\n        FirstDict = {\"Po\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4}\n        SecondDict = {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5}\n        ThirdDict = {\"NA\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4}\n        for d in [FirstDict, SecondDict, ThirdDict]:\n            if set(X) == set(d): \n                return d\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        def get_label(t):\n            return self.orderDict[t]\n        return np.array([get_label(n) for n in X])","metadata":{"id":"tnU0VQpKoni7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some Neighborhood seems to be more expensive than the others so..:","metadata":{"id":"sciCKtNYoni7"}},{"cell_type":"code","source":"class NeighborhoodTransformer(BaseEstimator, TransformerMixin):\n    neighborhoodsmap = {'StoneBr' : 2, 'NridgHt' : 2, 'NoRidge': 2, \n                        'MeadowV' : 0, 'IDOTRR' : 0, 'BrDale' : 0 ,\n                        'CollgCr': 1, 'Veenker' : 1, 'Crawfor' : 1,\n                        'Mitchel' : 1, 'Somerst' : 1, 'NWAmes' : 1,\n                        'OldTown' : 1, 'BrkSide' : 1, 'Sawyer' : 1, \n                        'NAmes' : 1, 'SawyerW' : 1, 'Edwards' : 1,\n                        'Timber' : 1, 'Gilbert' : 1, 'ClearCr' : 1,\n                        'NPkVill' : 1, 'Blmngtn' : 1, 'SWISU' : 1,\n                        'Blueste': 1}\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        def get_label(t):\n            return self.neighborhoodsmap[t]\n        return np.array([get_label(n) for [n] in X])","metadata":{"id":"cudnAJXAoni7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nAs I implemented previously specific transformers for categorical data, I will run the whole pipeline for them.","metadata":{"id":"K_-mq5-Poni7"}},{"cell_type":"code","source":"# Generating features:\norder_feats = [\"ExterQual\", \"ExterCond\", \"HeatingQC\", \"KitchenQual\", \"BsmtQual\", \n               \"BsmtCond\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\"]\n\noriginal_features_df = all_data[order_feats + ['Neighborhood']] # we need to save original values for one-hot encoding\n\norder_features = sklearn_pandas.gen_features(order_feats, [OrderedLabelTransformer])\nneighb_features = [(['Neighborhood'], [NeighborhoodTransformer()])]\n\n# Pipeline\nlabel_encoder = sklearn_pandas.DataFrameMapper(neighb_features + order_features)\n\n# The list with order of column names\ncols = [\"Neighborhood\"] + order_feats\n\n# Transformation both train and test set\ntransformed_feats = label_encoder.fit_transform(all_data)\n\n# Putting transformed features into dataframe\ntransformed_df = pd.DataFrame(transformed_feats, index=all_data.index, columns=cols)","metadata":{"id":"bWFtPk1Yoni8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_features_df.shape","metadata":{"id":"gFQnQmm_oni8","outputId":"5b5fdf3e-3d0a-4ec8-aabe-add5cd9b1c31"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature without any transformation till now\nrest_features = set(pd.concat([imputed_df, original_features_df],axis=1).columns).symmetric_difference(set(all_data.columns))\nrest_features_df = all_data[list(rest_features)]","metadata":{"id":"rumLJCHEoni8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = pd.concat([imputed_df, original_features_df, rest_features_df],axis=1)","metadata":{"id":"cqWWu2XSoni9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.shape","metadata":{"id":"_4q7inhroni9","outputId":"1e318040-5f76-4ac1-ecd5-fed9f5f70961"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{"id":"TXuYnbzconi9"}},{"cell_type":"markdown","source":"**Creating new features**","metadata":{"id":"Nslu7ttOoni9"}},{"cell_type":"markdown","source":"These features seem to be useful for house price prediction. As they are not contained in kaggle dataset I decided to create them from other informations.\n+ \"TotalSqrtFeet\" - Total Live Area  \n+ \"TotalBaths\" - Total Area for Bathrooms","metadata":{"id":"au7qXJ_zoni-"}},{"cell_type":"code","source":"# Total Squere Feet for house\nall_data[\"TotalSqrtFeet\"] = all_data[\"GrLivArea\"] + all_data[\"TotalBsmtSF\"]\n# test_df[\"TotalSqrtFeet\"] = test_df[\"GrLivArea\"] + test_df[\"TotalBsmtSF\"]\n\n# Total number of bathrooms\nall_data[\"TotalBaths\"] = all_data[\"BsmtFullBath\"] + (all_data[\"BsmtHalfBath\"]  * .5) + all_data[\"FullBath\"] + (all_data[\"HalfBath\"]* .5)\n# test_df[\"TotalBaths\"] = test_df[\"BsmtFullBath\"] + (test_df[\"BsmtHalfBath\"]  * .5) + test_df[\"FullBath\"] + (test_df[\"HalfBath\"]* .5)","metadata":{"id":"XCm3H3YXoni-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If the house has a garage\nall_data['Isgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n\n# If the house has a fireplace\nall_data['Isfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n# If the house has a pool\nall_data['Ispool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\n# If the house has second floor\nall_data['Issecondfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\n# If the house has Open Porch\nall_data['IsOpenPorch'] = all_data['OpenPorchSF'].apply(lambda x: 1 if x > 0 else 0)\n\n# If the house has Wood Deck\nall_data['IsWoodDeck'] = all_data['WoodDeckSF'].apply(lambda x: 1 if x > 0 else 0)","metadata":{"id":"XppXwtIjoni-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**One-hot encoding**","metadata":{"id":"O0P8ikrgoni-"}},{"cell_type":"code","source":"all_data = all_data.drop([\"SalePrice\"], axis = 1)\n\nhot_one_features = pd.get_dummies(all_data).reset_index(drop=True)\nhot_one_features.shape","metadata":{"id":"_LGO-kFDoni-","outputId":"a78c7333-a404-40f0-8fb5-10d758f488e3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = pd.concat([transformed_df, hot_one_features],axis=1)","metadata":{"id":"N1CMCIINoni_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting into train/test","metadata":{"id":"7_A2kiCKoni_"}},{"cell_type":"code","source":"train_preprocessed = all_data.iloc[:len(data_df),:]\ntest_preprocessed = all_data.iloc[len(train_preprocessed):,:]\nprint(len(test_preprocessed) == len(test_df))","metadata":{"id":"r57RLJ3joni_","outputId":"077ff332-a62e-4d1e-8687-948a93824906"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelization\n-----------------------------------","metadata":{"id":"Wl6QJZ7HonjA"}},{"cell_type":"code","source":"X_train = train_preprocessed","metadata":{"id":"ixGWzqVVonjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\nfrom sklearn.linear_model import ElasticNet, Lasso, ElasticNetCV\nfrom sklearn.ensemble import  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import RegressorMixin\nimport lightgbm as lgb","metadata":{"id":"hFYkwQWZonjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Environment specification:\\n')\nfor mod in sklearn, xgb, lgb:\n    print(mod.__name__, mod.__version__)","metadata":{"id":"_k4sOahhonjA","outputId":"71b33912-db07-4af6-e8b8-7d0c0aaf56ca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Defining scoring function**","metadata":{"id":"cT6uFm90onjB"}},{"cell_type":"code","source":"def rmse(model):\n    n_folds=5\n    kfold = KFold(n_folds, random_state=42, shuffle=True).get_n_splits(X_train)\n    rmse_score = np.sqrt(-cross_val_score(model, X_train, y_train, scoring = \"neg_mean_squared_error\", cv = kfold, verbose = -1, n_jobs=-1))\n    return(np.mean(rmse_score))","metadata":{"id":"uxS0gEv5onjB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Linear Regression**","metadata":{"id":"WAYPcwTtonjB"}},{"cell_type":"code","source":"lr_model = make_pipeline(RobustScaler(), LinearRegression()) #TODO: why Robust Scaler?\n\nlr_model.fit(X_train, y_train)\ny_train_pred = lr_model.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)\n\nprint(\"Mean Squared Error = {:.8f}\".format(MSE_train))\nprint(\"RMSE score for Linear Regression: {:.3f}\".format(rmse(lr_model)))","metadata":{"id":"F-pZvDP_onjB","outputId":"4700cd44-d9bf-44e9-8e23-ab169082238f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.5)\nplt.figure(figsize=(10,6))\nsns.scatterplot(y_train, y_train_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs. Predicted Prices\")","metadata":{"id":"nT_lFs2UonjC","outputId":"8f647c4b-6513-416c-cc58-ba9ed4feb4fd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Residual plot - result should be randomly located around the 0 value\nplt.figure(figsize=(10,6))\nsns.scatterplot(y_train_pred, y_train_pred - y_train)\nplt.title(\"Residual Plot\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")","metadata":{"id":"cXx8A392onjC","outputId":"74e509cc-249d-48a8-e748-97a8c5e3a8d4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LASSO model**","metadata":{"id":"q-g4s4nTonjD"}},{"cell_type":"code","source":"lasso_model = make_pipeline(RobustScaler(), \n                         LassoCV(alphas = [0.0004, 0.0005, 0.0006],\n                                 random_state = 0,\n                                 cv = 10))\n\nlasso_model.fit(X_train, y_train)\n\ny_train_pred = lasso_model.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)\n\n# print(\"Best alpha : {}\", lasso_model.alpha_)\nprint(\"Mean Squared Error = {:.8f}\".format(MSE_train))\nprint(\"RMSE score for LASSO: {:.3f}\".format(rmse(lasso_model)))","metadata":{"id":"NQwB4r0EonjD","outputId":"95a71a4a-2b61-46f3-fa95-d2d8c13e92f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting predictions\nplt.figure(figsize=(10,6))\nsns.scatterplot(y_train_pred, y_train)\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted Prices\")\nplt.ylabel(\"Real  Prices\")\nplt.show","metadata":{"id":"Hx5QXhqEonjD","outputId":"afbfde4b-4330-4f82-a1af-dcd85d57c2e5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GradientBoostingRegressor**","metadata":{"id":"VuhKikrSonjD"}},{"cell_type":"code","source":"gbr = GradientBoostingRegressor(random_state=0)\nparam_grid = {'n_estimators': [2500],\n              'max_features': [13],\n              'max_depth': [5],\n              'learning_rate': [0.05],\n              'subsample': [0.8],\n             'random_state' : [5]}\n                              \ngb_model = GridSearchCV(estimator=gbr, param_grid=param_grid, n_jobs=1, cv=5)\ngb_model.fit(X_train, y_train)","metadata":{"id":"0yxsm-0ConjE","outputId":"a5487d36-e5eb-4f93-f867-bf39766a0c73"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = gb_model.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)\nprint('Best Parameters: {}'.format(gb_model.best_params_))\nprint(\"Mean Squared Error = {:.8f}\".format(MSE_train))\nprint(\"RMSE score for GB: {:.3f}\".format(rmse(gb_model)))","metadata":{"id":"BwAgfIIponjE","outputId":"05032397-5ffa-4cb5-a246-8477e27b4c91"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**XGB Regressor**","metadata":{"id":"sBVRmGy4onjE"}},{"cell_type":"markdown","source":"**ElasticNet**","metadata":{"id":"8ctJWGjzonjF"}},{"cell_type":"code","source":"en_model = ElasticNetCV(alphas = [0.0001, 0.0003, 0.0004, 0.0006], \n                        l1_ratio = [.9, .92], \n                        random_state = 0,\n                        cv=10)\n","metadata":{"id":"qMUSTTKfonjF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_model.fit(X_train, y_train)\n\ny_train_pred = en_model.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)","metadata":{"id":"R2TPMXPMonjG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean Squared Error = {:.8f}\".format(MSE_train))\nprint(\"RMSE score for ElasticNet: {:.3f}\".format(rmse(en_model)))","metadata":{"id":"34kH499NonjG","outputId":"20b4e34e-282d-4dcc-b8a1-467109b5efec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"OGt4qITdonjG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LightGBM**","metadata":{"id":"eh3cYTS-onjH"}},{"cell_type":"code","source":"lgb_model = lgb.LGBMRegressor(objective='regression', num_leaves=5,\n                              learning_rate=0.05, n_estimators=800,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nlgb_model.fit(X_train, y_train)\n\ny_train_pred = lgb_model.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)\n\nprint(\"Mean Squared Error = {:.8f}\".format(MSE_train))\nprint(\"RMSE score for LGBMRegressor: {:.4f}\".format(rmse(lgb_model)))","metadata":{"id":"5SqGd2xuonjH","outputId":"ef6bf628-63e1-463e-8fee-42673dbc65ad"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BaggingRegressor**","metadata":{"id":"wr3wleTWonjH"}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingRegressor\n\nmodel2 = sklearn.ensemble.BaggingRegressor(base_estimator = en_model, n_estimators = 50, \n                                           max_samples = 30, max_features = 200, verbose = 3, n_jobs = 3)\nmodel2.fit(X_train, y_train)\n\ny_train_pred = model2.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)\n\nprint(\"Mean Squared Error = {:.8f}\".format(MSE_train))","metadata":{"id":"LCDQDqYdonjH","outputId":"b1ec9080-21d4-4b9d-85d6-2dca4cb53536"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"RMSE score for BaggingRegressor: {:.4f}\".format(rmse(model2)))","metadata":{"id":"xzb7AVpaonjI","outputId":"5365fb99-f267-40a4-8ba2-a547ea0eef61"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stacking","metadata":{"id":"M9bVW0gTonjI"}},{"cell_type":"code","source":"from mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\n","metadata":{"id":"AtFjyARQonjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso_model = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas = [0.0005],\n                              random_state = 42, cv=5))\n\nelasticnet_model = make_pipeline(RobustScaler(), \n                           ElasticNetCV(max_iter=1e7, alphas=[0.0005], \n                                        cv=5, l1_ratio=0.9))\n\nlgbm_model = make_pipeline(RobustScaler(),\n                        lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                                      learning_rate=0.05, n_estimators=800,\n                                      max_bin = 55, bagging_fraction = 0.8,\n                                      bagging_freq = 5, feature_fraction = 0.23,\n                                      feature_fraction_seed = 9, bagging_seed=9,\n                                      min_data_in_leaf = 6, \n                                      min_sum_hessian_in_leaf = 11))\n\nxgboost_model = make_pipeline(RobustScaler(),\n                        xgb.XGBRegressor(learning_rate = 0.01, n_estimators=3400, \n                                     max_depth=3,min_child_weight=0 ,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective= 'reg:linear',nthread=4,\n                                     scale_pos_weight=1,seed=27, \n                                     reg_alpha=0.00006))","metadata":{"id":"j9AGbm_9onjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_regressor = StackingCVRegressor(regressors=(lasso_model, elasticnet_model, lgbm_model), \n                               meta_regressor=lgbm_model, use_features_in_secondary=True)","metadata":{"id":"Sk6EfrJoonjJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Running** ","metadata":{"id":"VTP0rX3UonjJ"}},{"cell_type":"code","source":"stack_model = stack_regressor.fit(np.array(X_train),  np.array(y_train))","metadata":{"id":"0hLG0y-VonjJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_preds = en_model.predict(test_preprocessed)","metadata":{"id":"P5OYJIyQonjJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_gen_preds = stack_model.predict(test_preprocessed)","metadata":{"id":"AWsjGXononjK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_preds = lgb_model.predict(test_preprocessed)","metadata":{"id":"bqBNliuronjK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Weighted predictions\nstack_preds = ((0.25*en_preds) + (0.25*lgbm_preds) + (0.5*stack_gen_preds))","metadata":{"id":"aBUtWmVWonjK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df = pd.DataFrame(np.expm1(stack_preds), \n                              index = test_preprocessed.index+1, \n                              columns=[\"SalePrice\"])\npredictions_df.index.name = \"Id\"\npredictions_df.head()","metadata":{"id":"sgv_x-nXonjL","outputId":"d7723166-a049-4b36-c125-e9f4d277e070"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Kaggle Submission","metadata":{"id":"ztlDV2oconjL"}},{"cell_type":"code","source":"predictions_df[\"SalePrice\"].to_csv(\"my_predictions.csv\", header=True)","metadata":{"id":"5S4G1ZjBonjL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\nimport base64\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\ncreate_download_link(predictions_df)","metadata":{"id":"-1LT6ELyonjL","outputId":"42e55f8b-eb6b-4f75-9b33-a1c962c18fed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"wnX6SwzDujki"},"execution_count":null,"outputs":[]}]}